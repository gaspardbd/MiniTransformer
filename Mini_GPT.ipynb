{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "## Building a small GPT decoder in order to generate La Fontaine's Fables\n",
        "\n",
        "Inspired and learnt from Kartpathy's video on GPT :(https://karpathy.ai/zero-to-hero.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('laf.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "9322fefe-8c94-4e60-8bc4-563e489e27c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  534444\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "bbb75c94-b8fb-46dd-fd9f-cda3b83f1bd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Au lecteur\n",
            "\n",
            "  Cette version électronique reproduit dans son intégralité\n",
            "  la version originale.\n",
            "\n",
            "  La ponctuation n'a pas été modifiée hormis quelques corrections\n",
            "  mineures.\n",
            "\n",
            "  L'orthographe a été conservée. Seuls quelques mots ont été modifiés.\n",
            "  La liste des modifications se trouve à la fin du texte.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  FABLES\n",
            "\n",
            "  DE\n",
            "\n",
            "  LA FONTAINE\n",
            "\n",
            "\n",
            "  [Illustration: Buste de La Fontaine]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  FABLES\n",
            "\n",
            "  DE\n",
            "\n",
            "  LA FONTAINE\n",
            "\n",
            "\n",
            "  ILLUSTRATIONS\n",
            "\n",
            "  PAR\n",
            "\n",
            "  GRANDVILLE\n",
            "\n",
            "  [Illustration]\n",
            "\n",
            "\n",
            "  PARIS\n",
            "\n",
            "  GARNIER FRÈRES, LIBRAIRES-ÉDITEURS\n",
            "\n",
            "  6, RUE DES SAINTS-PÈRES--PALAIS-ROYAL, 215\n",
            "\n",
            "\n",
            "  M DCCC LXVIII\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[Illustration: A Monseigneur Le Dauphin]\n",
            "\n",
            "MONSEIGNEUR,\n",
            "\n",
            "\n",
            "S'IL y a quelque chose d'ingénieux dans la république des lettres,\n",
            "on peut dire que c'est la manière dont Ésope a débité sa morale. Il\n",
            "seroit véritablement à souhaiter que d'autres mains que les miennes y\n",
            "eussent ajouté les ornements de la poésie, puisque le plus sage des\n",
            "anciens a jugé qu'ils n'y étoient pas inutiles. J'ose, MONSEIGNEUR,\n",
            "vous en prése\n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "62f63572-3184-4294-ab7c-f88465e288cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !'()*,-.0123456789:;?ABCDEFGHIJLMNOPQRSTUVXYZ[]_abcdefghijklmnopqrstuvwxyz«»ÇÈÉÊÎàâæçèéêëîïôùûŒœήαβμπςτόϐἈ\n",
            "108\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "3c1390c0-ba81-4dff-e2ae-e6e47dd49760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[24, 64, 63, 59, 64, 70, 67]\n",
            "Bonjour\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"Bonjour\"))\n",
        "print(decode(encode(\"Bonjour\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "4f7d2d55-6bf7-40d8-8189-f6de3098cd17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([534444]) torch.int64\n",
            "tensor([ 1,  1, 23, 70,  1, 61, 54, 52, 69, 54, 70, 67,  0,  0,  1,  1, 25, 54,\n",
            "        69, 69, 54,  1, 71, 54, 67, 68, 58, 64, 63,  1, 88, 61, 54, 52, 69, 67,\n",
            "        64, 63, 58, 66, 70, 54,  1, 67, 54, 65, 67, 64, 53, 70, 58, 69,  1, 53,\n",
            "        50, 63, 68,  1, 68, 64, 63,  1, 58, 63, 69, 88, 56, 67, 50, 61, 58, 69,\n",
            "        88,  0,  1,  1, 61, 50,  1, 71, 54, 67, 68, 58, 64, 63,  1, 64, 67, 58,\n",
            "        56, 58, 63, 50, 61, 54,  9,  0,  0,  1,  1, 33, 50,  1, 65, 64, 63, 52,\n",
            "        69, 70, 50, 69, 58, 64, 63,  1, 63,  3, 50,  1, 65, 50, 68,  1, 88, 69,\n",
            "        88,  1, 62, 64, 53, 58, 55, 58, 88, 54,  1, 57, 64, 67, 62, 58, 68,  1,\n",
            "        66, 70, 54, 61, 66, 70, 54, 68,  1, 52, 64, 67, 67, 54, 52, 69, 58, 64,\n",
            "        63, 68,  0,  1,  1, 62, 58, 63, 54, 70, 67, 54, 68,  9,  0,  0,  1,  1,\n",
            "        33,  3, 64, 67, 69, 57, 64, 56, 67, 50, 65, 57, 54,  1, 50,  1, 88, 69,\n",
            "        88,  1, 52, 64, 63, 68, 54, 67, 71, 88, 54,  9,  1, 40, 54, 70, 61, 68,\n",
            "         1, 66, 70, 54, 61, 66, 70, 54, 68,  1, 62, 64, 69, 68,  1, 64, 63, 69,\n",
            "         1, 88, 69, 88,  1, 62, 64, 53, 58, 55, 58, 88, 68,  9,  0,  1,  1, 33,\n",
            "        50,  1, 61, 58, 68, 69, 54,  1, 53, 54, 68,  1, 62, 64, 53, 58, 55, 58,\n",
            "        52, 50, 69, 58, 64, 63, 68,  1, 68, 54,  1, 69, 67, 64, 70, 71, 54,  1,\n",
            "        83,  1, 61, 50,  1, 55, 58, 63,  1, 53, 70,  1, 69, 54, 73, 69, 54,  9,\n",
            "         0,  0,  0,  0,  0,  1,  1, 28, 23, 24, 33, 27, 40,  0,  0,  1,  1, 26,\n",
            "        27,  0,  0,  1,  1, 33, 23,  1, 28, 36, 35, 41, 23, 31, 35, 27,  0,  0,\n",
            "         0,  1,  1, 47, 31, 61, 61, 70, 68, 69, 67, 50, 69, 58, 64, 63, 20,  1,\n",
            "        24, 70, 68, 69, 54,  1, 53, 54,  1, 33, 50,  1, 28, 64, 63, 69, 50, 58,\n",
            "        63, 54, 48,  0,  0,  0,  0,  0,  1,  1, 28, 23, 24, 33, 27, 40,  0,  0,\n",
            "         1,  1, 26, 27,  0,  0,  1,  1, 33, 23,  1, 28, 36, 35, 41, 23, 31, 35,\n",
            "        27,  0,  0,  0,  1,  1, 31, 33, 33, 42, 40, 41, 39, 23, 41, 31, 36, 35,\n",
            "        40,  0,  0,  1,  1, 37, 23, 39,  0,  0,  1,  1, 29, 39, 23, 35, 26, 43,\n",
            "        31, 33, 33, 27,  0,  0,  1,  1, 47, 31, 61, 61, 70, 68, 69, 67, 50, 69,\n",
            "        58, 64, 63, 48,  0,  0,  0,  1,  1, 37, 23, 39, 31, 40,  0,  0,  1,  1,\n",
            "        29, 23, 39, 35, 31, 27, 39,  1, 28, 39, 79, 39, 27, 40,  7,  1, 33, 31,\n",
            "        24, 39, 23, 31, 39, 27, 40,  8, 80, 26, 31, 41, 27, 42, 39, 40,  0,  0,\n",
            "         1,  1, 16,  7,  1, 39, 42, 27,  1, 26, 27, 40,  1, 40, 23, 31, 35, 41,\n",
            "        40,  8, 37, 79, 39, 27, 40,  8,  8, 37, 23, 33, 23, 31, 40,  8, 39, 36,\n",
            "        45, 23, 33,  7,  1, 12, 11, 15,  0,  0,  0,  1,  1, 34,  1, 26, 25, 25,\n",
            "        25,  1, 33, 44, 43, 31, 31, 31,  0,  0,  0,  0,  0, 47, 31, 61, 61, 70,\n",
            "        68, 69, 67, 50, 69, 58, 64, 63, 20,  1, 23,  1, 34, 64, 63, 68, 54, 58,\n",
            "        56, 63, 54, 70, 67,  1, 33, 54,  1, 26, 50, 70, 65, 57, 58, 63, 48,  0,\n",
            "         0, 34, 36, 35, 40, 27, 31, 29, 35, 27, 42, 39,  7,  0,  0,  0, 40,  3,\n",
            "        31, 33,  1, 74,  1, 50,  1, 66, 70, 54, 61, 66, 70, 54,  1, 52, 57, 64,\n",
            "        68, 54,  1, 53,  3, 58, 63, 56, 88, 63, 58, 54, 70, 73,  1, 53, 50, 63,\n",
            "        68,  1, 61, 50,  1, 67, 88, 65, 70, 51, 61, 58, 66, 70, 54,  1, 53, 54,\n",
            "        68,  1, 61, 54, 69, 69, 67, 54, 68,  7,  0, 64, 63,  1, 65, 54, 70, 69,\n",
            "         1, 53, 58, 67, 54,  1, 66, 70, 54,  1, 52,  3, 54, 68, 69,  1, 61, 50,\n",
            "         1, 62, 50, 63, 58, 87, 67, 54,  1, 53, 64, 63, 69,  1, 80, 68, 64, 65,\n",
            "        54,  1, 50,  1, 53, 88, 51, 58, 69, 88,  1, 68, 50,  1, 62, 64, 67, 50,\n",
            "        61, 54,  9,  1, 31, 61,  0, 68, 54, 67, 64, 58, 69,  1, 71, 88, 67, 58,\n",
            "        69, 50, 51, 61, 54, 62, 54, 63, 69,  1, 83,  1, 68, 64, 70, 57, 50, 58,\n",
            "        69, 54, 67,  1, 66, 70, 54,  1, 53,  3, 50, 70, 69, 67, 54, 68,  1, 62,\n",
            "        50, 58, 63, 68,  1, 66, 70, 54,  1, 61, 54, 68,  1, 62, 58, 54, 63, 63,\n",
            "        54, 68,  1, 74,  0, 54, 70, 68, 68, 54, 63, 69,  1, 50, 59, 64, 70, 69,\n",
            "        88,  1, 61, 54, 68,  1, 64, 67, 63, 54, 62, 54, 63, 69, 68,  1, 53, 54,\n",
            "         1, 61, 50,  1, 65, 64, 88, 68, 58, 54,  7,  1, 65, 70, 58, 68, 66, 70,\n",
            "        54,  1, 61, 54,  1, 65, 61, 70, 68,  1, 68, 50, 56, 54,  1, 53, 54, 68,\n",
            "         0, 50, 63, 52, 58, 54, 63, 68,  1, 50,  1, 59, 70, 56, 88,  1, 66, 70,\n",
            "         3, 58, 61, 68,  1, 63,  3, 74,  1, 88, 69, 64, 58, 54, 63, 69,  1, 65,\n",
            "        50, 68,  1, 58, 63, 70, 69, 58, 61, 54, 68,  9,  1, 32,  3, 64, 68, 54,\n",
            "         7,  1, 34, 36, 35, 40, 27, 31, 29, 35, 27, 42, 39,  7,  0, 71, 64, 70,\n",
            "        68,  1, 54, 63,  1, 65, 67, 88, 68, 54])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "30ad5ef8-c9e4-4fcd-de94-2b14bb285a0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 1,  1, 23, 70,  1, 61, 54, 52, 69])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "60632e98-598d-4297-bbdb-25dfc2a8a853"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when input is tensor([1]) the target: 1\n",
            "when input is tensor([1, 1]) the target: 23\n",
            "when input is tensor([ 1,  1, 23]) the target: 70\n",
            "when input is tensor([ 1,  1, 23, 70]) the target: 1\n",
            "when input is tensor([ 1,  1, 23, 70,  1]) the target: 61\n",
            "when input is tensor([ 1,  1, 23, 70,  1, 61]) the target: 54\n",
            "when input is tensor([ 1,  1, 23, 70,  1, 61, 54]) the target: 52\n",
            "when input is tensor([ 1,  1, 23, 70,  1, 61, 54, 52]) the target: 69\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "e2b3a38d-017b-4b1c-9d78-a44c75aa34c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1,  1,  1,  1, 27, 69,  1, 71],\n",
            "        [ 1, 68, 54,  1, 71, 54, 67, 67],\n",
            "        [63, 69,  1, 54, 69,  1, 62, 50],\n",
            "        [52, 64, 70, 22,  0,  1,  1,  1]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1,  1,  1, 27, 69,  1, 71, 54],\n",
            "        [68, 54,  1, 71, 54, 67, 67, 50],\n",
            "        [69,  1, 54, 69,  1, 62, 50, 58],\n",
            "        [64, 70, 22,  0,  1,  1,  1,  1]])\n",
            "----\n",
            "when input is [1] the target: 1\n",
            "when input is [1, 1] the target: 1\n",
            "when input is [1, 1, 1] the target: 1\n",
            "when input is [1, 1, 1, 1] the target: 27\n",
            "when input is [1, 1, 1, 1, 27] the target: 69\n",
            "when input is [1, 1, 1, 1, 27, 69] the target: 1\n",
            "when input is [1, 1, 1, 1, 27, 69, 1] the target: 71\n",
            "when input is [1, 1, 1, 1, 27, 69, 1, 71] the target: 54\n",
            "when input is [1] the target: 68\n",
            "when input is [1, 68] the target: 54\n",
            "when input is [1, 68, 54] the target: 1\n",
            "when input is [1, 68, 54, 1] the target: 71\n",
            "when input is [1, 68, 54, 1, 71] the target: 54\n",
            "when input is [1, 68, 54, 1, 71, 54] the target: 67\n",
            "when input is [1, 68, 54, 1, 71, 54, 67] the target: 67\n",
            "when input is [1, 68, 54, 1, 71, 54, 67, 67] the target: 50\n",
            "when input is [63] the target: 69\n",
            "when input is [63, 69] the target: 1\n",
            "when input is [63, 69, 1] the target: 54\n",
            "when input is [63, 69, 1, 54] the target: 69\n",
            "when input is [63, 69, 1, 54, 69] the target: 1\n",
            "when input is [63, 69, 1, 54, 69, 1] the target: 62\n",
            "when input is [63, 69, 1, 54, 69, 1, 62] the target: 50\n",
            "when input is [63, 69, 1, 54, 69, 1, 62, 50] the target: 58\n",
            "when input is [52] the target: 64\n",
            "when input is [52, 64] the target: 70\n",
            "when input is [52, 64, 70] the target: 22\n",
            "when input is [52, 64, 70, 22] the target: 0\n",
            "when input is [52, 64, 70, 22, 0] the target: 1\n",
            "when input is [52, 64, 70, 22, 0, 1] the target: 1\n",
            "when input is [52, 64, 70, 22, 0, 1, 1] the target: 1\n",
            "when input is [52, 64, 70, 22, 0, 1, 1, 1] the target: 1\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "227c2195-809b-449f-a469-165a28342ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1,  1,  1,  1, 27, 69,  1, 71],\n",
            "        [ 1, 68, 54,  1, 71, 54, 67, 67],\n",
            "        [63, 69,  1, 54, 69,  1, 62, 50],\n",
            "        [52, 64, 70, 22,  0,  1,  1,  1]])\n"
          ]
        }
      ],
      "source": [
        "print(xb) # our input to the transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "59e5ee89-df9f-4598-ed33-6f708ec6beb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.997308 M parameters\n",
            "step 0: train loss 4.7935, val loss 4.7837\n",
            "step 500: train loss 1.6892, val loss 1.8160\n",
            "step 1000: train loss 1.4358, val loss 1.6734\n",
            "step 1500: train loss 1.3163, val loss 1.5428\n",
            "step 2000: train loss 1.2375, val loss 1.4166\n",
            "step 2500: train loss 1.1883, val loss 1.4123\n",
            "step 2999: train loss 1.1468, val loss 1.4251\n",
            "\n",
            "  [14] ANINIB.\n",
            "\n",
            "\n",
            "  [11] Entre assec: d'assa part gracer.\n",
            "\n",
            "  [Illustration]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  [Illustration]\n",
            "\n",
            "  XVIII\n",
            "\n",
            "  LA BOURGENME.\n",
            "\n",
            "\n",
            "  Est-il moins quelque saurieux,\n",
            "  Comment sens fit troute en le peau mordant,\n",
            "  Ulysses,\n",
            "        Souvent sur son tombe: et sauvel impilage:\n",
            "  Vous m'assissent anima6Mes leur œur, sous de plus instruiens,\n",
            "          Mais friers sur tous les accendentes:\n",
            "          Et mot sort vos autant sort été,\n",
            "          Qu'un peuple se veur maison,\n",
            "          En admestènelle un duisan ma visoit.\n",
            "\n",
            "  [Illustration]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  [Illustration]\n",
            "\n",
            "  XVII\n",
            "\n",
            "  LE PELLE ET LE FILHERVÉ.\n",
            "\n",
            "\n",
            "               Reposopit un enfant du phiebis.\n",
            "\n",
            "            La promemière à ramiteux;\n",
            "          Est fort en sans contrairiassent\n",
            "  Qu'il n'est plus fin chacuns qu'il vertin,\n",
            "            Je n'ai sais pas sur un prome.\n",
            "  LtO Et celle sans qui procure faire comme des hason;\n",
            "  Et l'auteur force, et jamande et que mis\n",
            "          Sentre les Dèves joie.\n",
            "  Il va sa foisille offriz, à la longter!\n",
            "\n",
            "  La tournisse bien lui que le trou de cettes perreur dans mort,\n",
            "  Rendit toujur le soupis\n",
            "               Toutenoit sa part à persuse,\n",
            "          Les pareilles de leurs écriance,\n",
            "  A ces meilles qu'ils n'y en pas tributes par là.\n",
            "  Vous vous approfitez quelqu sa néphante ayant pareille;\n",
            "  Le l'avoit vous entre quoi bâtiez enlement,\n",
            "          Passoit-nous de ami,\n",
            "          Que son seigneur, si pu dévote\n",
            "     Eut têté longtager d'abord,;\n",
            "          Témoignez que les répondites\n",
            "  Qui dans c'est clui le runde,\n",
            "  J'en vais de la force en sage ses jours pleines.\n",
            "  La pignon, du sef, se dit, aller s'autres bouts.\n",
            "  Sa sortre nous voisinance: Idevineur qu'il falloit mit à\n",
            "          Où noq ne fut prince à bâille-là que des lettres,\n",
            "          Nettenoissoit-il bien, ton point\n",
            "          Vensé d'abus, sans rendre de courage.\n",
            "  De rivitions suffit que suite: Ce qu'elle il\n",
            "          En faite parlantant donc\n",
            "          Regloiroit à ma fable la sans.\n",
            "  Il sont qu'ils pensoient bâtonné,\n",
            "  Qu'il renvoyoit triompés neci.\n",
            "\n",
            "  L'homme \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparamètres\n",
        "batch_size = 64  # combien de séquences indépendantes allons-nous traiter en parallèle ?\n",
        "block_size = 128  # quelle est la longueur de contexte maximale pour les prédictions ?\n",
        "max_iters = 3000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' si torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 200\n",
        "n_head = 5\n",
        "n_layer = 4\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Chargement du texte\n",
        "with open('laf.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Liste des caractères uniques dans le texte\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# Création d'un mapping des caractères vers des entiers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encodeur : prend une chaîne, renvoie une liste d'entiers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # décodeur : prend une liste d'entiers, renvoie une chaîne\n",
        "\n",
        "# Division des données en ensemble d'entraînement et de validation\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data)) # 90% pour l'entraînement, le reste pour la validation\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Chargement des lots de données\n",
        "def get_batch(split):\n",
        "    # génère un petit lot de données avec des entrées x et des cibles y\n",
        "    data = train_data si split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" une tête de self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # Calcul des scores d'attention (\"affinités\") en utilisant torch.einsum\n",
        "        wei = torch.einsum('btc,bsc->bts', q, k) * C**-0.5 # (B, T, T)  # NORMALISATIOn par sqrt(C)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "         # ou sinon:\n",
        "        # mask = torch.arange(T)[None, :] > torch.arange(T)[:, None]\n",
        "        # wei = wei.masked_fill(mask, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # Agrégation pondérée des valeurs\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = torch.einsum('bts,bsc->btc', wei, v) # (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" plusieurs têtes de self-attention en parallèle \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" une simple couche linéaire suivie d'une non-linéarité \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Bloc Transformer : communication suivie de calculs \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd : dimension des embeddings, n_head : nombre de têtes\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    \"\"\" Modèle de langage basé sur les bigrammes \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # chaque token lit directement les logits pour le token suivant à partir d'une table de lookup\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # normalisation finale\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx et targets sont tous deux des tenseurs (B,T) d'entiers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
        "        # idx est un tableau (B, T) des indices dans le contexte actuel\n",
        "        for _ in range(max_new_tokens):\n",
        "            # couper idx aux derniers block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # obtenir les prédictions\n",
        "            logits, loss = self(idx_cond) #(B,taille_phrase) vers (B,taille_phrase, vocab_size)\n",
        "            # ne se concentrer que sur la dernière étape temporelle\n",
        "            logits = logits[:, -1, :] / temperature # devient (B, C)\n",
        "            #PLUTOT QUE [:][-1][:]:  cela sélectionne le dernier élément du batch entier, et non pas le dernier token pour chaque élément du batch.\n",
        "            # appliquer softmax pour obtenir les probabilités\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # échantillonner à partir de la distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # ajouter l'indice échantillonné à la séquence en cours\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# imprimer le nombre de paramètres du modèle\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M paramètres')\n",
        "\n",
        "# créer un optimiseur PyTorch\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # de temps en temps, évaluer la perte sur les ensembles train et val\n",
        "    si iter % eval_interval == 0 ou iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"étape {iter}: perte d'entraînement {losses['train']:.4f}, perte de validation {losses['val']:.4f}\")\n",
        "\n",
        "    # échantillonner un lot de données\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # évaluer la perte\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# générer du texte à partir du modèle\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000, temperature=0.8)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vh8SeURf_Id",
        "outputId": "963b3b7d-f823-4dad-fa04-0228f25662f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 1.1432, val loss 1.4321\n",
            "step 500: train loss 1.1127, val loss 1.3801\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  [Illustration]\n",
            "\n",
            "  VII\n",
            "\n",
            "  L'ÉPHÉQUISSEUR.\n",
            "\n",
            "\n",
            "              Je mets essuice au mort,\n",
            "          Quelque ne croira pas menu;\n",
            "  Si des toiux que nous un bien part: cependant\n",
            "  Que n'est bâtir, ni qui galands qu'on monte.\n",
            "          Quand il est rat son éluté mangé,\n",
            "            Tous les champs d'affare Nécepter;\n",
            "  Je ponse à d'apporté, ôtant au vrai, veuger\n",
            " ,        Entre magistrabler, pillis des babors.\n",
            "  Je parle à moi dépense\n",
            "  A dit inventure; sa coquisable de ne plaisant alard,\n",
            "  Qu'à soi donnant à son moissant que je part trop beaucoupe\n",
            "  Tous aussi. Moi, je le blâme. Comme Prit à la vie,\n",
            "          Amour traînt de natirer,\n",
            "  Riant, gule-malle, et croyé d'exégère,\n",
            "          Qui se pand plus surpas, à moi,\n",
            "  Demeurer jeune le temps t'aille pays, une belle,\n",
            "  Qui, ferasse-pologuede? Xantus,\n",
            "  Ainsi, ne n'y parla va point aussitôt\n",
            "          Se seroit tous regassent et davantagré.\n",
            "          Le libre avec se prêts sage pas\n",
            "  Tous ont ignorant seulvent avec énéphants.\n",
            "  Pour une homme étatage, dans les zéphyres de traits\n",
            "  De duce voyons et ses ois, leurs loins auternent doux s'enfin,\n",
            "   Et donmier, sa déipresse apprave.\n",
            "  La chassé du magation, et peu le perdre trou,\n",
            "  Lui les femme en flattés du grand du Necté,\n",
            "          Ne pondra-t-on apparence; et,\n",
            "  Il eut entraînant lui démoir des paririens,\n",
            "\n",
            "  Et vous, puis cependant: Robe comme je ne t'amas\n",
            "          Remporter le baudet.\n",
            "  Si ce quue t'ouvrant pour mèque.\n",
            "  Au lit le Circédent de déput le créatur le mes à six.\n",
            "  Cela Mort il fit en chanter mon flatte,\n",
            "  Il ne devoit un rivage et notre glis:\n",
            "  Il est meilleurement toujours la nécession\n",
            "  Y donnence en comme la messire en évulence\n",
            "  Payamaché le lion. Natuader chose, marcais très-sauffaire,\n",
            "  Attre autrefois, et plus qu la brissita.\n",
            "  L'instarif aux petits de cours: hermés sur sa son voyage,\n",
            "  Vous êtes de Brander. Mangeons, et fin, et ne confin.\n",
            "  La chose rédulgent à cette exemple, un plus, aille lace\n",
            "  De trop avoit cent embarrassé, espèce bannassent va ma ma\n"
          ]
        }
      ],
      "source": [
        "for iter in range(1000):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igVna17Km0cE",
        "outputId": "9c4ee14e-217b-4065-f0f6-2cc344069d3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 1.0855, val loss 1.3901\n",
            "step 500: train loss 1.0622, val loss 1.3971\n",
            "\n",
            "succès d'un inspniteur\n",
            "          Passent-ils plairessant avoir plus de retemin\n",
            "  Esclavir d'autre difficul. Il fut bien\n",
            "  Que fournit pour tant si chacun s'embrassa d'un crimatière;\n",
            "  Sur quand la fille faitre, et l'objet, elle mit à l'or.\n",
            "  Nous délibarre ainsi donce, si philosophème.\n",
            "  [Illustration]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  [Illustration]\n",
            "\n",
            "  IV\n",
            "\n",
            "  LE PAROSSEUR.\n",
            "\n",
            "\n",
            "\n",
            "  n'étoit pas le coloup, de grandeur au gland:\n",
            "  Commez dit de la diluttre,\n",
            "  Et l'autre besoin, se plus-on son jour\n",
            "          Comme il pabusoit un testant,\n",
            "          Je le Jadis, qu'on peut chercher, pour  contre cet objet:\n",
            "  Tout au soin dur le vieillar faire à la Pinteur\n",
            "          Aillez viter. Ainsi pas craint:\n",
            "  Il ne me mette tout les âmes un auteure en seul?\n",
            "  Que m'affrère oblige avec trio que père:\n",
            "  Le bois, qui ne croit qu'on l'en avoit tombé:\n",
            "          Prend je récidissant pesé?\n",
            "  C'est là besoin, sitôt Plup-Repors, de façon repressenti,\n",
            "  De jamais dans l'avare et la moine chantemps\n",
            "        De bon sein, par que nos ont mérité\n",
            "            Des qualités choses nommoient les goireaux.\n",
            "  Tant, à chaque chacune nommère délicatience.\n",
            "  Là-dessus vous, vous, six je le maître (ces vont habines\n",
            "  Tout l'abdrage, mangeois et se meuvement;\n",
            "  Mais agiseau tenter qu'au seconnuer:\n",
            "  Sommes-moi lieu dit que Xantus s'acquitte impathro.\n",
            "\n",
            "\n",
            "  Deux faboles funes mevons, par la poie;\n",
            "  Il ne voyoit point à chans, tout le vent:\n",
            "          Il la diffente, on le voit dit:\n",
            "          Les réponidra son ennecter.\n",
            "  C'étoit d'en autour chemin le bon tour étendu,\n",
            "  Il s'ait crier aux hibou; Tu L'apportée\n",
            "   Dont ils étoient les doucerets,\n",
            "  Dans le bête rampabitoit qu'elle sentoit,\n",
            "          Mais à votre un amis frère polige.\n",
            "  T'arrive dut cet empire, pour il n'a raignir,\n",
            "  Chacun courut que son homme recourt à la personne\n",
            "  Ne maisonge y jamai le Dieu d'Uroit:\n",
            "  Quand l'un jour traiste nos deux camarades enfants:\n",
            "          Je ne mais forme par d'écureillir\n",
            "  Ne saure demeurez ces bouchards pour cette?\n",
            "  L'un: que Le singe a raisonne grand\n"
          ]
        }
      ],
      "source": [
        "for iter in range(1000):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
